# -*- coding: utf-8 -*-
"""
Queue Scenarios ‚Äî Demos do sistema de fila/messaging.

Port dos demos originais relacionados ao FileBasedJobQueue e
sistema de mensageria.
"""

from __future__ import annotations

import asyncio
import gc
import subprocess
import sys
import tempfile
from datetime import datetime
from os import getenv
from pathlib import Path

from kernel import Result
from runtime.demo.base import (
    BaseDemo,
    DemoCategory,
    DemoContext,
    DemoLifecycle,
    DemoFlow,
    DemoFlowType,
    DemoResult,
)
from runtime.demo.registry import DemoRegistry


@DemoRegistry.register
class QueueE2EDemo(BaseDemo):
    """
    Demo E2E REAL - FakeGitHubAgent ‚Üí Webhook REAL ‚Üí Worker ‚Üí JobQueue.

    Demonstra o fluxo completo REAL de ponta a ponta:
    1. FakeGitHubAgent cria issues REAIS no GitHub
    2. GitHub envia webhooks REAIS para nosso servidor (via ngrok)
    3. WebhookProcessor processa e enfileira jobs
    4. Worker processa jobs automaticamente
    5. Verifica m√©tricas finais em /metrics

    PR√â-REQUISITOS:
    - API Skybridge rodando (python -m apps.api.main)
    - Ngrok ativo e configurado no GitHub webhook
    - GITHUB_TOKEN configurado
    - GITHUB_REPO configurado

    Relacionado √†: PRD018 Fase 2 (SQLite Job Queue)
    """

    demo_id = "queue-e2e"
    demo_name = "Queue E2E Demo - REAL Flow (GitHub ‚Üí Webhook ‚Üí Worker)"
    description = "Demo E2E REAL: issues reais ‚Üí webhooks reais ‚Üí worker ‚Üí SQLite"
    category = DemoCategory.QUEUE
    required_configs = ["GITHUB_TOKEN", "GITHUB_REPO"]
    estimated_duration_seconds = 60
    tags = ["queue", "api", "worker", "sqlite", "e2e", "real", "github"]
    related_issues = [55]
    lifecycle = DemoLifecycle.STABLE
    last_reviewed = "2026-01-22"

    def define_flow(self) -> DemoFlow:
        return DemoFlow(
            flow_type=DemoFlowType.JOB_EXECUTION,
            description="Demonstra√ß√£o E2E REAL com FakeGitHubAgent + Webhooks REAIS + Worker + SQLite",
            actors=[
                "FakeGitHubAgent",
                "GitHub API",
                "GitHub Webhook System",
                "Ngrok Tunnel",
                "WebhookReceiver (/webhooks/github)",
                "WebhookProcessor",
                "SQLiteJobQueue",
                "WebhookWorker (background thread)",
                "/metrics endpoint",
            ],
            steps=[
                "Configurar ambiente",
                "Verificar que API est√° rodando",
                "Verificar m√©tricas iniciais",
                "FakeGitHubAgent cria issues REAIS no GitHub",
                "GitHub envia webhooks REAIS via ngrok",
                "Aguardar worker processar jobs",
                "Verificar m√©tricas finais e conclus√£o",
            ],
            entry_point="cli",
            expected_outcome="Issues reais criadas ‚Üí webhooks reais recebidos ‚Üí jobs processados ‚Üí m√©tricas atualizadas",
        )

    async def validate_prerequisites(self) -> Result[None, str]:
        """Verifica pr√©-requisitos: API rodando e configs GitHub."""
        try:
            import httpx

            # Verifica API
            async with httpx.AsyncClient() as client:
                response = await client.get("http://127.0.0.1:8000/health", timeout=2.0)
                if response.status_code != 200:
                    return Result.err("API n√£o est√° saud√°vel - execute: python -m apps.api.main")

            # Verifica configs GitHub
            github_token = getenv("GITHUB_TOKEN")
            github_repo = getenv("GITHUB_REPO")

            if not github_token:
                return Result.err("GITHUB_TOKEN n√£o configurado")

            if not github_repo:
                return Result.err("GITHUB_REPO n√£o configurado (ex: hadst/skybridge-refactor-events)")

            return Result.ok(None)
        except Exception:
            return Result.err("API n√£o est√° rodando - execute: python -m apps.api.main")

    async def run(self, context: DemoContext) -> DemoResult:
        import httpx
        import os
        import time

        from core.agents.mock.fake_github_agent import (
            FakeGitHubAgent,
            RealisticIssueTemplates,
        )

        # Configura√ß√£o
        api_url = "http://127.0.0.1:8000"
        github_token = getenv("GITHUB_TOKEN")
        github_repo = getenv("GITHUB_REPO")

        owner, name = github_repo.split("/", 1)

        self.log_info(f"API URL: {api_url}")
        self.log_info(f"GitHub Repo: {github_repo}")
        self.log_info(f"Queue Type: SQLiteJobQueue (via JOB_QUEUE_PROVIDER env)")

        # ============================================================
        # PASSO 1: Configurar ambiente
        # ============================================================

        self.log_progress(1, 7, "Configurando ambiente...")

        # Configura env para usar SQLite
        os.environ["JOB_QUEUE_PROVIDER"] = "sqlite"
        os.environ["SQLITE_DB_PATH"] = "data/jobs.db"

        self.log_success("Ambiente configurado")

        # ============================================================
        # PASSO 2: Verificar que API est√° rodando
        # ============================================================

        self.log_progress(2, 7, "Verificando que API est√° rodando...")

        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{api_url}/health", timeout=5.0)
                if response.status_code == 200:
                    self.log_success(f"API est√° rodando: {response.json()['status']}")
                else:
                    return DemoResult.error(f"API retornou status {response.status_code}")
        except Exception as e:
            return DemoResult.error(f"N√£o foi poss√≠vel conectar na API: {e}")

        # ============================================================
        # PASSO 3: Verificar m√©tricas iniciais
        # ============================================================

        self.log_progress(3, 7, "Verificando m√©tricas iniciais...")

        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{api_url}/metrics", timeout=30.0)
                if response.status_code == 200:
                    metrics_data = response.json()
                    if metrics_data.get("ok"):
                        metrics_initial = metrics_data["metrics"]
                        queue_type = metrics_data.get("queue_type", "Unknown")
                        self.log_info(f"Queue Type: {queue_type}")
                        self.log_info(f"Queue Size: {metrics_initial['queue_size']}")
                        self.log_info(f"Total Enqueued: {metrics_initial.get('total_enqueued', 0)}")
                else:
                    self.log_warning(f"/metrics retornou status {response.status_code}")
        except Exception as e:
            self.log_warning(f"Erro ao obter m√©tricas iniciais: {e}")

        # ============================================================
        # PASSO 4: FakeGitHubAgent cria issues REAIS no GitHub
        # ============================================================

        self.log_progress(4, 7, "FakeGitHubAgent criando issues REAIS no GitHub...")

        templates = RealisticIssueTemplates()

        # Criar 3 issues realistas
        issues_to_create = [
            templates.webhook_deduplication_bug(),
            templates.trello_integration_feature(),
            templates.agent_orchestrator_refactor(),
        ]

        created_issues = []

        try:
            async with FakeGitHubAgent(owner, name, github_token) as agent:
                for i, issue_template in enumerate(issues_to_create):
                    self.log_info(f"Criando issue {i+1}/3: {issue_template.title[:60]}...")

                    response = await agent.create_issue(issue_template)

                    if response:
                        data = response.json()
                        issue_number = data["number"]
                        issue_url = data["html_url"]
                        created_issues.append((issue_number, issue_url))

                        self.log_success(f"Issue #{issue_number} criada: {issue_url}")
                    else:
                        self.log_warning(f"Falha ao criar issue {i+1}/3")

                    # Delay para n√£o rate limit (GitHub limit: ~300 req/hour)
                    if i < len(issues_to_create) - 1:
                        await asyncio.sleep(2)

        except Exception as e:
            return DemoResult.error(f"Erro ao criar issues no GitHub: {e}")

        self.log_success(f"{len(created_issues)} issues criadas com sucesso!")

        # ============================================================
        # PASSO 5: GitHub envia webhooks REAIS via ngrok
        # ============================================================

        self.log_progress(5, 7, "GitHub enviando webhooks REAIS via ngrok...")

        self.log_info("üí° O GitHub est√° enviando webhooks AUTOMATICAMENTE para seu servidor")
        self.log_info("üí° Webhook: GitHub ‚Üí ngrok ‚Üí API Skybridge ‚Üí WebhookProcessor")
        self.log_info("üí° Jobs est√£o sendo enfileirados em SQLiteJobQueue")

        # Pequena pausa para webhooks chegarem
        await asyncio.sleep(3)

        # ============================================================
        # PASSO 6: Aguardar worker processar jobs
        # ============================================================

        self.log_progress(6, 7, "Aguardando worker processar jobs...")

        # Poll /metrics at√© jobs serem processados
        max_wait = 60  # segundos
        start_wait = time.time()
        completed_count = 0

        self.log_info("üìä Monitorando /metrics para acompanhar processamento...")

        while time.time() - start_wait < max_wait:
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(f"{api_url}/metrics", timeout=5.0)
                    if response.status_code == 200:
                        metrics_data = response.json()
                        if metrics_data.get("ok"):
                            metrics = metrics_data["metrics"]
                            completed = metrics.get("completed", 0)
                            queue_size = metrics.get("queue_size", 0)
                            processing = metrics.get("processing", 0)

                            self.log_info(f"Queue: {queue_size} | Processing: {processing} | Completed: {completed}")

                            # Nota: jobs podem n√£o ser processados se n√£o houver skill configurado
                            # O worker pode "pular" jobs que n√£o requerem execu√ß√£o de agente
                            if completed >= 3 or (queue_size == 0 and completed >= 0):
                                completed_count = completed
                                self.log_success("Worker processou todos os jobs!")
                                break
            except Exception as e:
                self.log_warning(f"Erro ao verificar m√©tricas: {e}")

            await asyncio.sleep(2)

        if completed_count < 3:
            self.log_warning(f"Jobs processados: {completed_count}/3 (alguns jobs podem n√£o requerer execu√ß√£o)")

        # ============================================================
        # PASSO 7: Verificar m√©tricas finais
        # ============================================================

        self.log_progress(7, 7, "Verificando m√©tricas finais...")

        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{api_url}/metrics", timeout=10.0)
                if response.status_code == 200:
                    metrics_data = response.json()
                    if metrics_data.get("ok"):
                        metrics_final = metrics_data["metrics"]

                        self.log_separator("‚îÄ")
                        self.log_info("M√âTRICAS FINAIS:")
                        self.log_info(f"  Queue Size: {metrics_final['queue_size']}")
                        self.log_info(f"  Processing: {metrics_final.get('processing', 0)}")
                        self.log_info(f"  Completed: {metrics_final.get('completed', 0)}")
                        self.log_info(f"  Failed: {metrics_final.get('failed', 0)}")
                        self.log_info(f"  Total Enqueued: {metrics_final.get('total_enqueued', 0)}")
                        self.log_info(f"  Queue Type: {metrics_data.get('queue_type', 'Unknown')}")
                        self.log_separator("‚îÄ")
        except Exception as e:
            self.log_warning(f"Erro ao obter m√©tricas finais: {e}")

        self.log_separator("‚ïê")
        self.log_success("Demo E2E REAL conclu√≠da!")
        self.log_separator("‚ïê")

        self.log_info("üìã Issues criadas no GitHub:")
        for number, url in created_issues:
            self.log_info(f"   Issue #{number}: {url}")

        self.log_info("")
        self.log_info("üí° O que aconteceu:")
        self.log_info("   1. FakeGitHubAgent criou issues REAIS no GitHub")
        self.log_info("   2. GitHub enviou webhooks REAIS via ngrok")
        self.log_info("   3. API Skybridge recebeu e processou webhooks")
        self.log_info("   4. Worker processou jobs (ou marcou como skip)")
        self.log_info("   5. SQLiteJobQueue persistiu tudo")

        return DemoResult.success(
            message="E2E REAL funcionando: GitHub ‚Üí Webhook ‚Üí Worker ‚Üí SQLite",
            github_repo=github_repo,
            issues_created=created_issues,
            jobs_completed=completed_count,
        )


@DemoRegistry.register
class SQLitePersistenceNoRestartDemo(BaseDemo):
    """
    Demo de Persist√™ncia SQLite - Sem Restart (Fluxo Real).

    Demonstra o fluxo REAL completo usando:
    - WebhookProcessor (real)
    - JobOrchestrator (real)
    - SQLiteJobQueue em data/jobs.db
    - Webhooks reais via API

    Fluxo:
    1. Enviar 3 webhooks reais para a API
    2. Verificar que jobs foram enfileirados
    3. Processar jobs via JobOrchestrator
    4. Verificar m√©tricas finais

    Relacionado √†: PRD018 Fase 2 (SQLite Job Queue)
    """

    demo_id = "sqlite-persistence-no-restart"
    demo_name = "SQLite Persistence Demo - No Restart (Real Flow)"
    description = "Demo de persist√™ncia SQLite sem restart (fluxo real com API)"
    category = DemoCategory.QUEUE
    required_configs = []
    estimated_duration_seconds = 30
    tags = ["sqlite", "persistence", "queue", "fase2", "real-flow"]
    related_issues = [55]
    lifecycle = DemoLifecycle.STABLE
    last_reviewed = "2026-01-22"

    def define_flow(self) -> DemoFlow:
        return DemoFlow(
            flow_type=DemoFlowType.JOB_EXECUTION,
            description="Demonstra√ß√£o de persist√™ncia SQLite sem restart (fluxo real)",
            actors=["WebhookProcessor", "SQLiteJobQueue", "JobOrchestrator", "API"],
            steps=[
                "Configurar SQLiteJobQueue real",
                "Enviar 3 webhooks via WebhookProcessor",
                "Verificar jobs enfileirados",
                "Processar jobs via JobOrchestrator",
                "Verificar m√©tricas finais",
            ],
            entry_point="cli",
            expected_outcome="Jobs processados com persist√™ncia em SQLite (fluxo real)",
        )

    async def validate_prerequisites(self) -> Result[None, str]:
        # Verifica depend√™ncias
        try:
            from infra.webhooks.adapters.job_queue_factory import JobQueueFactory
            from core.webhooks.application.webhook_processor import WebhookProcessor
            from core.webhooks.application.job_orchestrator import JobOrchestrator
            return Result.ok(None)
        except ImportError as e:
            return Result.err(f"Depend√™ncia faltando: {e}")

    async def run(self, context: DemoContext) -> DemoResult:
        import httpx
        from infra.webhooks.adapters.job_queue_factory import JobQueueFactory
        from core.webhooks.application.webhook_processor import WebhookProcessor
        from core.webhooks.application.job_orchestrator import JobOrchestrator
        from infra.domain_events.in_memory_event_bus import InMemoryEventBus
        from pathlib import Path
        import time

        # Configura√ß√£o
        db_path = "data/jobs.db"
        api_url = "http://localhost:8000"

        self.log_info(f"Banco SQLite: {db_path}")
        self.log_info(f"API URL: {api_url}")

        # 1. Setup: Criar diret√≥rio e limpar banco anterior
        self.log_progress(1, 6, "Configurando ambiente...")

        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)

        db_file = Path(db_path)
        if db_file.exists():
            self.log_info(f"Removendo banco anterior: {db_path}")
            db_file.unlink()

        # 2. Inicializar componentes reais
        self.log_progress(2, 6, "Inicializando WebhookProcessor e JobOrchestrator...")

        job_queue = JobQueueFactory.create_from_env()
        event_bus = InMemoryEventBus()

        processor = WebhookProcessor(job_queue=job_queue, event_bus=event_bus)
        orchestrator = JobOrchestrator(job_queue=job_queue, event_bus=event_bus, worktree_manager=None)

        self.log_success("Componentes inicializados")
        self.log_info(f"Job Queue: {type(job_queue).__name__}")

        # 3. Enviar 3 webhooks reais via WebhookProcessor
        self.log_progress(3, 6, "Enviando 3 webhooks via WebhookProcessor...")

        delivery_ids = []
        for i in range(3):
            delivery_id = f"demo-no-restart-{i}-{int(time.time() * 1000)}"
            delivery_ids.append(delivery_id)

            payload = {
                "action": "opened",
                "issue": {
                    "number": 1000 + i,
                    "title": f"[DEMO] SQLite Persistence Test {i}",
                    "body": f"Testando persist√™ncia SQLite - Job {i}",
                    "user": {"login": "demo-user"},
                },
                "repository": {
                    "name": "skybridge-refactor-events",
                    "owner": {"login": "hadst"},
                },
            }

            result = await processor.process_github_issue(
                payload=payload,
                event_type="issues.opened",
                delivery_id=delivery_id,
            )

            if result.is_ok:
                job_id = result.value
                self.log_info(f"Webhook {i+1}/3 processado: job_id={job_id}")
            else:
                return DemoResult.error(f"Erro ao processar webhook {i+1}: {result.error}")

        self.log_success("3 webhooks enviados")

        # 4. Verificar jobs enfileirados
        self.log_progress(4, 6, "Verificando jobs enfileirados...")

        size = job_queue.size()
        metrics = await job_queue.get_metrics()

        self.log_info(f"Queue Size: {size}")
        self.log_info(f"Queue Size (metrics): {metrics['queue_size']}")
        self.log_info(f"Total Enqueued: {metrics['total_enqueued']}")

        if size != 3:
            return DemoResult.error(f"Esperado 3 jobs na fila, obtido {size}")

        self.log_success("3 jobs confirmados na fila")

        # 5. Processar jobs via JobOrchestrator (simula worker)
        self.log_progress(5, 6, "Processando jobs via JobOrchestrator...")

        processed = 0
        for i in range(3):
            # Desenfileira job
            job = await job_queue.dequeue(timeout_seconds=2.0)
            if job:
                self.log_info(f"Job {i+1}/3 desenfileirado: {job.job_id}")
                self.log_info(f"   Issue: #{job.event.payload.get('issue', {}).get('number')}")

                # Simula processamento (sem executar agente real)
                await asyncio.sleep(0.1)

                # Marca como completado
                await job_queue.complete(job.job_id)

                self.log_success(f"Job {i+1}/3 completado: {job.job_id}")
                processed += 1
            else:
                self.log_warning(f"Timeout aguardando job {i+1}/3")

        if processed != 3:
            return DemoResult.error(f"Esperado processar 3 jobs, obtido {processed}")

        # 6. M√©tricas finais
        self.log_progress(6, 6, "Coletando m√©tricas finais...")

        metrics_final = await job_queue.get_metrics()

        self.log_separator("‚îÄ")
        self.log_info("M√âTRICAS FINAIS:")
        self.log_info(f"  Queue Size: {metrics_final['queue_size']}")
        self.log_info(f"  Completed: {metrics_final['completed']}")
        self.log_info(f"  Total Enqueued: {metrics_final['total_enqueued']}")
        self.log_separator("‚îÄ")

        # Verifica√ß√µes
        if metrics_final['queue_size'] != 0:
            return DemoResult.error(f"Queue size deveria ser 0: {metrics_final['queue_size']}")
        if metrics_final['completed'] != 3:
            return DemoResult.error(f"Completed incorreto: {metrics_final['completed']}")

        # Cleanup
        await event_bus.close()

        self.log_success("Demo conclu√≠da com sucesso!")

        return DemoResult.success(
            message="SQLiteJobQueue sem restart funcionando corretamente (fluxo real)",
            db_path=db_path,
            total_enqueued=metrics_final['total_enqueued'],
            completed=metrics_final['completed'],
        )


@DemoRegistry.register
class SQLitePersistenceWithRestartDemo(BaseDemo):
    """
    Demo de Persist√™ncia SQLite - Com Restart (Fluxo Real).

    Demonstra que jobs persistem atrav√©s de restarts reais do aplicativo.

    Fluxo:
    1. FASE 1: Enviar 2 webhooks via WebhookProcessor
    2. FASE 2: Destruir componentes (simular app shutdown)
    3. FASE 3: Recriar componentes (simular app restart)
    4. FASE 4: Verificar que jobs persistiram no banco
    5. FASE 5: Processar jobs ap√≥s restart

    Relacionado √†: PRD018 Fase 2 (SQLite Job Queue)
    """

    demo_id = "sqlite-persistence-with-restart"
    demo_name = "SQLite Persistence Demo - With Restart (Real Flow)"
    description = "Demo de persist√™ncia SQLite com restart (fluxo real)"
    category = DemoCategory.QUEUE
    required_configs = []
    estimated_duration_seconds = 30
    tags = ["sqlite", "persistence", "restart", "queue", "fase2", "real-flow"]
    related_issues = [55]
    lifecycle = DemoLifecycle.STABLE
    last_reviewed = "2026-01-22"

    def define_flow(self) -> DemoFlow:
        return DemoFlow(
            flow_type=DemoFlowType.JOB_EXECUTION,
            description="Demonstra√ß√£o de persist√™ncia SQLite com restart (fluxo real)",
            actors=["WebhookProcessor (antes)", "App Shutdown", "App Restart", "WebhookProcessor (depois)", "Worker"],
            steps=[
                "FASE 1: Enviar 2 webhooks via WebhookProcessor",
                "FASE 2: Destruir componentes (shutdown)",
                "FASE 3: Recriar componentes (restart)",
                "FASE 4: Verificar persist√™ncia de jobs",
                "FASE 5: Processar jobs ap√≥s restart",
            ],
            entry_point="cli",
            expected_outcome="Jobs persistem atrav√©s do restart real",
        )

    async def validate_prerequisites(self) -> Result[None, str]:
        try:
            from infra.webhooks.adapters.job_queue_factory import JobQueueFactory
            from core.webhooks.application.webhook_processor import WebhookProcessor
            from core.webhooks.application.job_orchestrator import JobOrchestrator
            from infra.domain_events.in_memory_event_bus import InMemoryEventBus
            return Result.ok(None)
        except ImportError as e:
            return Result.err(f"Depend√™ncia faltando: {e}")

    async def run(self, context: DemoContext) -> DemoResult:
        from infra.webhooks.adapters.job_queue_factory import JobQueueFactory
        from core.webhooks.application.webhook_processor import WebhookProcessor
        from core.webhooks.application.job_orchestrator import JobOrchestrator
        from infra.domain_events.in_memory_event_bus import InMemoryEventBus
        from pathlib import Path
        import time

        # Configura√ß√£o
        db_path = "data/jobs.db"

        self.log_info(f"Banco SQLite: {db_path}")
        self.log_info(f"Queue Type: SQLiteJobQueue (via JobQueueFactory)")

        # ============================================================
        # SETUP INICIAL
        # ============================================================

        data_dir = Path("data")
        data_dir.mkdir(exist_ok=True)

        db_file = Path(db_path)
        if db_file.exists():
            self.log_info(f"Removendo banco anterior: {db_path}")
            db_file.unlink()

        # ============================================================
        # FASE 1: ANTES DO RESTART - Enfileirar jobs
        # ============================================================

        self.log_separator("=")
        self.log_info("FASE 1: ANTES DO RESTART")
        self.log_separator("=")

        self.log_progress(1, 5, "Inicializando componentes (antes do restart)...")

        job_queue_before = JobQueueFactory.create_from_env()
        event_bus_before = InMemoryEventBus()

        processor_before = WebhookProcessor(job_queue=job_queue_before, event_bus=event_bus_before)

        self.log_success("Componentes inicializados")
        self.log_info(f"Job Queue: {type(job_queue_before).__name__}")

        # Enfileirar 2 jobs
        self.log_progress(2, 5, "Enfileirando 2 jobs antes do restart...")

        delivery_ids = []
        for i in range(2):
            delivery_id = f"demo-with-restart-{i}-{int(time.time() * 1000)}"
            delivery_ids.append(delivery_id)

            payload = {
                "action": "opened",
                "issue": {
                    "number": 2000 + i,
                    "title": f"[DEMO-RESTART] SQLite Persistence Test {i}",
                    "body": f"This job survives restart {i}",
                    "user": {"login": "demo-user"},
                },
                "repository": {
                    "name": "skybridge-refactor-events",
                    "owner": {"login": "hadst"},
                },
            }

            result = await processor_before.process_github_issue(
                payload=payload,
                event_type="issues.opened",
                delivery_id=delivery_id,
            )

            if result.is_ok:
                job_id = result.value
                self.log_info(f"Webhook {i+1}/2 processado: job_id={job_id}")
            else:
                return DemoResult.error(f"Erro ao processar webhook {i+1}: {result.error}")

        # Verificar m√©tricas
        size_before = job_queue_before.size()
        metrics_before = await job_queue_before.get_metrics()

        self.log_success("2 jobs enfileirados")
        self.log_info(f"Queue Size: {size_before}")
        self.log_info(f"M√©tricas: queue_size={metrics_before['queue_size']}, total_enqueued={metrics_before['total_enqueued']}")

        # ============================================================
        # FASE 2: SIMULAR RESTART (Shutdown)
        # ============================================================

        self.log_separator("=")
        self.log_info("FASE 2: SIMULANDO RESTART (SHUTDOWN)")
        self.log_separator("=")

        self.log_progress(3, 5, "Destruindo componentes (simulando shutdown)...")

        # Fecha event bus
        await event_bus_before.close()

        # Destroi todos os componentes (simula app sendo desligado)
        del processor_before
        del job_queue_before
        del event_bus_before
        gc.collect()

        self.log_success("Componentes destru√≠dos (app simulou shutdown)")
        self.log_info("‚è≥ Pausa de 1 segundo para simular tempo de restart...")
        await asyncio.sleep(1)

        # ============================================================
        # FASE 3: DEPOIS DO RESTART - Recriar componentes
        # ============================================================

        self.log_separator("=")
        self.log_info("FASE 3: DEPOIS DO RESTART")
        self.log_separator("=")

        self.log_progress(4, 5, "Recriando componentes (ap√≥s restart)...")

        # Recria componentes (simula app iniciando novamente)
        job_queue_after = JobQueueFactory.create_from_env()
        event_bus_after = InMemoryEventBus()

        processor_after = WebhookProcessor(job_queue=job_queue_after, event_bus=event_bus_after)

        self.log_success("Componentes recriados (app iniciou novamente)")
        self.log_info(f"Job Queue: {type(job_queue_after).__name__}")

        # ============================================================
        # FASE 4: VERIFICAR PERSIST√äNCIA
        # ============================================================

        # Verificar que jobs ainda est√£o l√°
        size_after = job_queue_after.size()
        metrics_after = await job_queue_after.get_metrics()

        self.log_separator("‚îÄ")
        self.log_info("VERIFICA√á√ÉO DE PERSIST√äNCIA:")
        self.log_info(f"  Tamanho da fila: {size_after}")
        self.log_info(f"  Queue Size: {metrics_after['queue_size']}")
        self.log_info(f"  Total Enqueued: {metrics_after['total_enqueued']}")
        self.log_separator("‚îÄ")

        # Verifica√ß√µes cr√≠ticas de persist√™ncia
        if size_after != 2:
            return DemoResult.error(f"PERSIST√äNCIA FALHOU: Esperado 2 jobs, obtido {size_after}")
        if metrics_after['queue_size'] != 2:
            return DemoResult.error(f"PERSIST√äNCIA FALHOU: queue_size={metrics_after['queue_size']}")
        if metrics_after['total_enqueued'] != 2:
            return DemoResult.error(f"PERSIST√äNCIA FALHOU: total_enqueued={metrics_after['total_enqueued']}")

        self.log_success("‚úì PERSIST√äNCIA CONFIRMADA: Jobs sobreviveram ao restart!")

        # ============================================================
        # FASE 5: PROCESSAR JOBS AP√ìS RESTART
        # ============================================================

        self.log_progress(5, 5, "Processando jobs ap√≥s restart...")

        processed = 0
        for i in range(2):
            # Desenfileira job
            job = await job_queue_after.dequeue(timeout_seconds=2.0)
            if job:
                self.log_info(f"Job {i+1}/2 desenfileirado: {job.job_id}")
                self.log_info(f"   Issue: #{job.event.payload.get('issue', {}).get('number')}")
                self.log_info(f"   Title: {job.event.payload.get('issue', {}).get('title')}")

                # Simula processamento
                await asyncio.sleep(0.1)

                # Marca como completado
                await job_queue_after.complete(job.job_id)

                self.log_success(f"Job {i+1}/2 completado: {job.job_id}")
                processed += 1
            else:
                self.log_warning(f"Timeout aguardando job {i+1}/2")

        if processed != 2:
            return DemoResult.error(f"Esperado processar 2 jobs, obtido {processed}")

        # M√©tricas finais
        metrics_final = await job_queue_after.get_metrics()

        self.log_separator("‚îÄ")
        self.log_info("M√âTRICAS FINAIS:")
        self.log_info(f"  Queue Size: {metrics_final['queue_size']}")
        self.log_info(f"  Completed: {metrics_final['completed']}")
        self.log_info(f"  Processing: {metrics_final['processing']}")
        self.log_separator("‚îÄ")

        # Verifica√ß√µes finais
        if metrics_final['completed'] != 2:
            return DemoResult.error(f"Completed incorreto: {metrics_final['completed']}")
        if metrics_final['processing'] != 0:
            return DemoResult.error(f"Processing deveria ser 0: {metrics_final['processing']}")
        if metrics_final['queue_size'] != 0:
            return DemoResult.error(f"Queue size deveria ser 0: {metrics_final['queue_size']}")

        # Cleanup
        await event_bus_after.close()

        self.log_success("Demo conclu√≠da com sucesso!")
        self.log_success("‚úì Jobs persistiram atrav√©s do restart real")
        self.log_success("‚úì Jobs foram processados corretamente ap√≥s restart")

        return DemoResult.success(
            message="SQLiteJobQueue com restart funcionando corretamente (fluxo real)",
            db_path=db_path,
            jobs_survived=metrics_after['queue_size'],
            jobs_processed=metrics_final['completed'],
        )
